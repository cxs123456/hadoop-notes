## 1. Spark Streaming 介绍
	### 1.1 Spark Streaming是什么
	SparkStreaming是流处理框架（小批量流处理，不是实时流处理）

	** Spark Streaming 具有很好的整合性 **
	* Spark Streaming 可以从 Kafka, Flume, TCP 等流和队列中获取数据
	* Spark Streaming 可以将处理过的数据写入文件系统, 常见数据库中

	> **SparkStreaming 的特点：**
	* Spark Streaming 会源源不断的处理数据, 称之为流计算。
	* Spark Streaming 并不是实时流, 而是按照时间切分小批量, 一个一个的小批量处理。
	* Spark Streaming 是流计算, 所以可以理解为数据会源源不断的来, 需要长时间运行。

	### 1.2 Spark Streaming使用
	``` scala
		// 创建 StreamingContext
		val conf = new SparkConf().setAppName(appName).setMaster(master)
		val ssc = new StreamingContext(conf, Seconds(1))
		val lines: DStream[String] = ssc.socketTextStream(
		  hostname = args(0),
		  port = args(1).toInt,
		  storageLevel = StorageLevel.MEMORY_AND_DISK_SER)

		val words: DStream[String] = lines
		  .flatMap(_.split(" "))
		  .map(x => (x, 1))
		  .reduceByKey(_ + _)
		words.print()                                                 
		ssc.start()                                                        
		ssc.awaitTermination() 
	```
	> **总结：**
	1.StreamingContext 是 Spark Streaming 程序的入口
	2.Spark Streaming 的处理机制叫做小批量, 英文叫做 mini-batch, 是收集了一定时间的数据后生成 RDD。
		new StreamingContext(sparkConf, Seconds(1)) 这段代码中的第二个参数指定批次生成的时间
	3.Spark Streaming 中的编程模型叫做 DStream, 所有的 API 都从 DStream 开始。 
		其作用就类似于 RDD可以理解为 DStream 是一个管道, 数据源源不断的从这个管道进去, 被处理, 再出去


	### 1.3 Spark Streaming的容错
	**1.热备：**
	这行代码中的 StorageLevel.MEMORY_AND_DISK_SER 的作用是什么? 其实就是热备份
	* 当 Receiver 获取到数据要存储的时候, 是交给 BlockManager 存储的
	* 如果设置了 StorageLevel.MEMORY_AND_DISK_SER, 则意味着 BlockManager 不仅会在本机存储, 也会发往其它的主机进行存储, 本质就是冗余备份
	* 如果某一个计算失败了, 通过冗余的备份, 再次进行计算即可

	**2.冷备：**
	冷备在 Spark Streaming 中的手段叫做 WAL (预写日志)
	当 Receiver 获取到数据后, 会交给 BlockManager 存储
	在存储之前先写到 WAL 中, WAL 中保存了 Redo Log, 其实就是记录了数据怎么产生的, 以便于恢复的时候通过 Log 恢复
	当出错的时候, 通过 Redo Log 去重放数据

	**3.重放：**
	* 有一些上游的外部系统是支持重放的, 比如说 Kafka。
	* Kafka 可以根据 Offset 来获取数据。
	* 当 SparkStreaming 处理过程中出错了, 只需要通过 Kafka 再次读取即可。

---
## 2. Structured Streaming 介绍
	**Structured Streaming 是 Spark Streaming 的进化版，可以实时流处理。**

	### 2.1 RDD、DataFrame和Dataset的优缺点
	__1. 编程模型 RDD 的优点和缺陷 __
	* 针对自定义数据对象进行处理, 可以处理任意类型的对象, 比较符合面向对象
	* RDD 无法感知到数据的结构, 无法针对数据结构进行编程

	__2. 编程模型 DataFrame 的优点和缺陷 __
	* DataFrame 保留有数据的元信息, API 针对数据的结构进行处理, 例如说可以根据数据的某一列进行排序或者分组
	* DataFrame 在执行的时候会经过 Catalyst 进行优化, 并且序列化更加高效, 性能会更好
	* DataFrame 只能处理结构化的数据, 无法处理非结构化的数据, 因为 DataFrame 的内部使用 Row 对象保存数据
	* Spark 为 DataFrame 设计了新的数据读写框架, 更加强大, 支持的数据源众多

	__3. 编程模型 Dataset 的优点和缺陷 __
	* Dataset 结合了 RDD 和 DataFrame 的特点, 从 API 上即可以处理结构化数据, 也可以处理非结构化数据
	* Dataset 和 DataFrame 其实是一个东西, 所以 DataFrame 的性能优势, 在 Dataset 上也有


	### 2.2 Structured Streaming中的序列化
	__1.RDD 的序列化只能使用 Java 序列化器, 或者 Kryo 序列化器 __
		conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") conf.registerKryoClasses(Array(classOf[Person]))
	__2.DataFrame 和 Dataset 中的序列化 __
		使用 Spark 特有的序列化协议, 生成 UnsafeInternalRow 用以保存数据,这样不仅能减少数据量, 也能减少序列化和反序列化的开销, 
		其速度大概能达到 RDD 的序列化的 20 倍左右。


	### 2.3 Structured Streaming的使用
	**1.需求：**
	- 编写一个流式计算的应用, 不断的接收外部系统的消息。
	- 对消息中的单词进行词频统计。
	- 统计全局的结果。
	**1.代码实现：**
	``` scala
		// 1. 创建 SparkSession
		val spark = SparkSession.builder()
		  .master("local[6]")
		  .appName("socket_processor")
		  .getOrCreate()

		spark.sparkContext.setLogLevel("ERROR")   

		import spark.implicits._

		// 2. 读取外部数据源, 并转为 Dataset[String]
		val source = spark.readStream
		  .format("socket")
		  .option("host", "127.0.0.1")
		  .option("port", 9999)
		  .load()
		  .as[String]                             

		// 3. 统计词频
		val words = source.flatMap(_.split(" "))
		  .map((_, 1))
		  .groupByKey(_._1)
		  .count()

		// 4. 输出结果
		words.writeStream
		  .outputMode(OutputMode.Complete())      
		  .format("console")                      
		  .start()                                
		  .awaitTermination()   


	```

	> **总结：**
	1.Structured Streaming 中的编程步骤依然是先读, 后处理, 最后落地。
	2.Structured Streaming 中的编程模型依然是 DataFrame 和 Dataset。
	3.Structured Streaming 中依然是有外部数据源读写框架的, 叫做 readStream 和 writeStream。
	4.Structured Streaming 和 SparkSQL 几乎没有区别, 唯一的区别是, readStream 读出来的是流, 
		writeStream 是将流输出, 而 SparkSQL 中的批处理使用 read 和 write。

	### 2.4 Structured Streaming的体系结构
	**Dataset 不仅可以表达流式数据的处理, 也可以表达批量数据的处理。**
	**Dataset 之所以可以表达流式数据的处理, 因为 Dataset 可以模拟一张无限扩展的表, 外部的数据会不断的流入到其中。**

	**在 Structured Streaming 中负责整体流程和执行的驱动引擎叫做 StreamExecution.**
	StreamExecution 是整个 Structured Streaming 的核心, 负责在流上的查询
	**StreamExecution 中三个重要的组成部分：**
	- Source，负责读取每个批量的数据。
	- Sink，负责将结果写入外部数据源。
	- Logical Plan，负责针对每个小批量生成逻辑执行计划。

	__增量查询__
	StreamExecution 中使用 StateStore 来进行状态的维护
	在 Structured Streaming 中有一个全局范围的高可用 StateStore, 这个时候针对增量的查询变为如下步骤：
	1. 从 StateStore 中取出上次执行完成后的状态
	2. 把上次执行的结果加入本批次, 再进行计算, 得出全局结果
	3. 将当前批次的结果放入 StateStore 中, 留待下次使用

	> **总结：**
	1.StreamExecution 是整个 Structured Streaming 的核心, 负责在流上的查询
	2.StreamExecution 中三个重要的组成部分, 分别是:
			Source 负责读取每个批量的数据, 
			Sink 负责将结果写入外部数据源, 
			Logical Plan 负责针对每个小批量生成执行计划
	3.StreamExecution 中使用 StateStore 来进行状态的维护


	（有时候再复习一遍下面的：）
	Source
	Sink
	有状态算子







